learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lgb.cv(
params = params,
data = lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
return(opt_results)
}
# Usage example
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
cv <- lightgbm::lgb.cv(
params = params,
data = lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "binary",
metric = "binary_logloss",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
return(opt_results)
}
# Usage example
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
return(opt_results)
}
# Usage example
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 2)
?svm
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 2)
best_params <- getBestPars(opt_results)
best_params <- ParBayesianOptimization::getBestPars(opt_results)
best_params
opt_results
all_results <- ParBayesianOptimization::getScore(opt_results)
opt_results$scoreSummary
param_grid
best_params
opt_results$scoreSummary
best_params
opt_results$scoreSummary %>% dim()
all_results <- ParBayesianOptimization::getScore(opt_results)
opt_results$scoreSummary %>% dim()
opt_results$scoreSummary
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
browser()
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 2)
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
browser()
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
return(opt_results)
}
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 2)
cv
cv$best_score
best_params
source("~/.active-rstudio-document")
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
# browser()
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
browser()
return(opt_results)
}
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 2)
opt_results
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(param_grid$num_leaves[1]),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(param_grid$num_leaves[1]),
max_depth = as.integer(param_grid$max_depth[1]),
learning_rate = param_grid$learning_rate[1],
n_estimators = as.integer(param_grid$n_estimators[1]),
min_data_in_leaf = as.integer(param_grid$min_data_in_leaf[1]),
feature_fraction = param_grid$feature_fraction[1],
lambda_l1 = param_grid$lambda_l1[1],
lambda_l2 = param_grid$lambda_l2[1],
verbose = -1
)
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
)cv
cv
all_cv_results <- as.data.table(cv_results$record_evals$valid$binary_logloss$eval)
all_cv_results <- data.table::as.data.table(cv_results$record_evals$valid$binary_logloss$eval)
all_cv_results <- data.table::as.data.table(cv$record_evals$valid$binary_logloss$eval)
all_cv_results <- data.table::as.data.table(cv$record_evals$valid$binary_logloss$eval)
all_cv_results
cv$record_evals$valid$rmse
all_cv_results <- data.table::as.data.table(cv$record_evals$valid$rmse)
all_cv_results
all_cv_results <- data.table::as.data.table(cv$record_evals$valid$binary_logloss$eval)params
params
cv$best_iter
cv$best_score
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
# params <- list(
#   objective = "regression",
#   metric = "rmse",
#   num_leaves = as.integer(param_grid$num_leaves[1]),
#   max_depth = as.integer(param_grid$max_depth[1]),
#   learning_rate = param_grid$learning_rate[1],
#   n_estimators = as.integer(param_grid$n_estimators[1]),
#   min_data_in_leaf = as.integer(param_grid$min_data_in_leaf[1]),
#   feature_fraction = param_grid$feature_fraction[1],
#   lambda_l1 = param_grid$lambda_l1[1],
#   lambda_l2 = param_grid$lambda_l2[1],
#   verbose = -1
# )
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
# all_cv_results <- data.table::as.data.table(cv$record_evals$valid$rmse)
# all_cv_results
# cv$record_evals$valid$rmse
# cv$best_iter
# cv$best_score
# browser()
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
n_iter
source("~/.active-rstudio-document")
# Bayesian optimization for LightGBM
lightgbm_bayesian_optimization <- function(X, y, n_iter = 30) {
# Define parameter bounds
bounds <- list(
num_leaves = c(15L, 255L),
max_depth = c(3L, 12L),
learning_rate = c(0.01, 0.3),
n_estimators = c(100L, 1000L),
min_data_in_leaf = c(5L, 100L),
feature_fraction = c(0.5, 1.0),
lambda_l1 = c(0, 10),
lambda_l2 = c(0, 10)
)
# Objective function
obj_func <- function(num_leaves, max_depth, learning_rate,
n_estimators, min_data_in_leaf,
feature_fraction, lambda_l1, lambda_l2) {
params <- list(
objective = "regression",
metric = "rmse",
num_leaves = as.integer(num_leaves),
max_depth = as.integer(max_depth),
learning_rate = learning_rate,
n_estimators = as.integer(n_estimators),
min_data_in_leaf = as.integer(min_data_in_leaf),
feature_fraction = feature_fraction,
lambda_l1 = lambda_l1,
lambda_l2 = lambda_l2,
verbose = -1
)
# params <- list(
#   objective = "regression",
#   metric = "rmse",
#   num_leaves = as.integer(param_grid$num_leaves[1]),
#   max_depth = as.integer(param_grid$max_depth[1]),
#   learning_rate = param_grid$learning_rate[1],
#   n_estimators = as.integer(param_grid$n_estimators[1]),
#   min_data_in_leaf = as.integer(param_grid$min_data_in_leaf[1]),
#   feature_fraction = param_grid$feature_fraction[1],
#   lambda_l1 = param_grid$lambda_l1[1],
#   lambda_l2 = param_grid$lambda_l2[1],
#   verbose = -1
# )
cv <- lightgbm::lgb.cv(
params = params,
data = lightgbm::lgb.Dataset(data = as.matrix(X), label = y),
nfold = 5,
early_stopping_rounds = 20
)
# all_cv_results <- data.table::as.data.table(cv$record_evals$valid$rmse)
# all_cv_results
# cv$record_evals$valid$rmse
# cv$best_iter
# cv$best_score
# browser()
return(list(Score = -cv$best_score))  # Negative because we want to minimize loss
}
# Run optimization
opt_results <- ParBayesianOptimization::bayesOpt(
FUN = obj_func,
bounds = bounds,
initPoints = 10,
iters.n = n_iter,
acq = "ei"
)
browser()
return(opt_results)
}
# Usage example
# opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 30)
opt_results <- lightgbm_bayesian_optimization(train, y_train, n_iter = 1)
opt_results
all_results <- ParBayesianOptimization::getScore(opt_results)
# Define parameter grid for grid search
param_grid <- expand.grid(
num_leaves = c(15, 31, 63, 127),          # Controls model complexity
max_depth = c(3, 5, 7, 9, -1),           # -1 means no limit
learning_rate = c(0.01, 0.05, 0.1, 0.2), # Step size shrinkage
n_estimators = c(100, 200, 500),         # Number of boosting iterations
min_data_in_leaf = c(20, 50, 100),       # Minimum samples in leaf
feature_fraction = c(0.6, 0.8, 1.0),     # Fraction of features used
bagging_fraction = c(0.6, 0.8, 1.0),     # Fraction of data used
bagging_freq = c(0, 5, 10),              # Frequency for bagging
lambda_l1 = c(0, 0.1, 1, 10),            # L1 regularization
lambda_l2 = c(0, 0.1, 1, 10)             # L2 regularization
)
param_grid
data
data %>% filter(Target == 'GDF15')
